{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu_backward(z):\n",
    "    return (z > 0).astype(int)\n",
    "\n",
    "def tanh_backward(z):\n",
    "    return 1 - z**2\n",
    "\n",
    "def initialize_deep_layers(layers_dims):\n",
    "    \n",
    "    #with he -initilization but have to find out why it works better than random initialization\n",
    "    \n",
    "    L = len(layers_dims)\n",
    "    parameters = {}\n",
    "    \n",
    "    for i in range(1,L):\n",
    "        \n",
    "        parameters[f'W{i}'] = np.random.randn(layers_dims[i], layers_dims[i-1]) * np.sqrt(1/layers_dims[i-1])\n",
    "        parameters[f'b{i}'] = np.zeros((layers_dims[i],1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def L_backward_propopgation(parameters, cache, X, Y, activation='tanh', lambd=0.0):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "    grads = {}\n",
    "    \n",
    "    for i in range(L,0,-1):\n",
    "        if i == L:\n",
    "            #assummption last layers activation func is sigmoid\n",
    "            #otherwise remove this block make changes in grad func in next block according to your choice\n",
    "            grads[f'dZ{i}'] = cache[f'A{i}'] - Y\n",
    "            \n",
    "        else:\n",
    "            grads[f'dA{i}'] = np.dot(parameters[f'W{i+1}'].T, grads[f'dZ{i+1}'] )\n",
    "            \n",
    "            grad_func = relu_backward if activation == 'relu' else tanh_backward\n",
    "            \n",
    "            grads[f'dZ{i}'] = grads[f'dA{i}'] * grad_func( cache[f'A{i}'] )\n",
    "            \n",
    "        if i != 1 :\n",
    "            grads[f'dW{i}'] = 1./m * np.dot(grads[f'dZ{i}'] ,cache[f'A{i-1}'].T) \n",
    "        else:\n",
    "            grads[f'dW{i}'] = 1./m * np.dot(grads[f'dZ{i}'] ,X.T)\n",
    "            \n",
    "        grads[f'dW{i}'] = grads[f'dW{i}'] + (lambd/m) *(parameters[f'W{i}'])\n",
    "        \n",
    "        grads[f'db{i}'] = 1./m * np.sum(grads[f'dZ{i}'] ,axis=1, keepdims=True)\n",
    "        \n",
    "    return grads\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = initialize_deep_layers([3,4,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid( z ) :\n",
    "    return 1 / ( 1 + np.exp( -z ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_feed_forward(paramters, X):\n",
    "    \n",
    "    L = len(paramters) //2\n",
    "    \n",
    "    cache = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        \n",
    "        if l==1:\n",
    "            cache[f'Z{l}'] = np.dot(paramters[f'W{l}'],X) + paramters[f'b{l}']\n",
    "        else:\n",
    "            cache[f'Z{l}'] = np.dot(paramters[f'W{l}'],cache[f'A{l-1}']) + paramters[f'b{l}']\n",
    "            \n",
    "        if l!=L:\n",
    "            cache[f'A{l}'] = np.tanh(cache[f'Z{l}'])\n",
    "        else:\n",
    "            cache[f'A{L}'] = sigmoid(cache[f'Z{l}'])\n",
    "\n",
    "    return cache[f'A{L}'] , cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL , Y ,parameters, lambd =0.0):\n",
    "    \n",
    "    #if lambd == 0 then it should mean that you dont want to use regularization\n",
    "    \n",
    "    m= Y.shape[1]\n",
    "    \n",
    "    logprobs = ( Y * np.log( AL ) ) + ( ( 1 - Y ) * np.log( 1 - AL ) )\n",
    "    cost = -1./m * np.sum(logprobs)\n",
    "    cost = float(np.squeeze(cost))\n",
    "    \n",
    "    regularized_cost = 0\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for i in range(1,L+1):\n",
    "        \n",
    "        regularized_cost += np.sum(np.square(parameters[f'W{i}']))\n",
    "        \n",
    "    regularized_cost = (lambd / (2*m) ) * regularized_cost\n",
    "    \n",
    "    return cost + regularized_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.01):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    for i in range(1,L+1):\n",
    "        parameters[f'W{i}'] = parameters[f'W{i}'] - learning_rate* (grads[f'dW{i}'])\n",
    "        parameters[f'b{i}'] = parameters[f'b{i}'] - learning_rate* (grads[f'db{i}'])\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,Y,layers_dims,learning_rate=0.01, lambd=0.0, num_iterations = 200, print_cost_every_n_steps=1):\n",
    "    print(lambd,'lambda')\n",
    "    parameters = initialize_deep_layers(layers_dims)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        AL,cache = L_feed_forward(parameters,X)\n",
    "\n",
    "        cost = compute_cost(AL, Y, parameters,lambd=lambd)\n",
    "        \n",
    "        if i % print_cost_every_n_steps == 0:\n",
    "            print(f'cost at iteration {i} = {cost}' )\n",
    "\n",
    "        grads = L_backward_propopgation(parameters, cache, X, Y,lambd=lambd)\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_dataset = pd.read_csv( '~/NLP/NLP_Disaster_tweets/train.csv')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer( 'english')\n",
    "\n",
    "def num_remover( val):\n",
    "    tokens = val.split()\n",
    "    nums = [ str(i) for i in range(10)]\n",
    "    final_tokens = []\n",
    "    for token in tokens:\n",
    "        token = token.strip()\n",
    "        if not any( token.startswith( num ) for num in nums):\n",
    "            final_tokens.append(token)\n",
    "    return ' '.join(final_tokens)\n",
    "\n",
    "\n",
    "def replace_urls(text):\n",
    "    tokens = text.split()\n",
    "    \n",
    "    final_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token.lower().startswith('http'):\n",
    "            final_tokens.append('url')\n",
    "        elif token.lower().startswith('@'):\n",
    "            final_tokens.append('taggeduser')\n",
    "        else:\n",
    "            final_tokens.append(token)\n",
    "    return ' '.join(final_tokens)\n",
    "\n",
    "\n",
    "def clean_text(df):\n",
    "\n",
    "    replace_words = [ '&amp' , 'and' , '#' ]\n",
    "\n",
    "    df['text'] = df['text'].apply(replace_urls)\n",
    "\n",
    "    for word in replace_words :\n",
    "        df[ 'text' ] = df[ 'text' ].str.replace( word , '' )\n",
    "\n",
    "    df[ 'text' ] = df['text' ].apply( lambda txt : ' '.join( stemmer.stem(lemmatizer.lemmatize( word ) ) for word in txt.split( ' ') ) )\n",
    "\n",
    "    df['keyword'] = df['keyword'].fillna('').str.replace('%20' , ' ')\n",
    "    df[ 'text' ] = df.apply( lambda row : str( row[ 'text' ] ) + ' ' + str(row[ 'keyword' ]) if row[ 'keyword' ] else row[ 'text' ] , axis = 1)\n",
    "\n",
    "    df['text'] = df['text'].apply(num_remover)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class MyDataTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.count_vectorizer = CountVectorizer()\n",
    "        self.standard_scaler = StandardScaler()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        x = clean_text(X)\n",
    "        x = self.count_vectorizer.fit_transform(X['text'])\n",
    "        x = x.toarray()\n",
    "        x = self.standard_scaler.fit(x)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        x = clean_text(X)\n",
    "        x = self.count_vectorizer.transform(X['text'])\n",
    "        x = x.toarray()\n",
    "        x = self.standard_scaler.transform(x)\n",
    "        x = x.T\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv( '~/NLP/NLP_Disaster_tweets/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(train_df, train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_transformer = MyDataTransformer()\n",
    "\n",
    "X_train = my_data_transformer.fit_transform(X_train)\n",
    "\n",
    "y_train = np.array(y_train).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers_dims = [X_train.shape[0],256, 128, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 lambda\n",
      "cost at iteration 0 = 0.7572016821141467\n",
      "cost at iteration 1 = 0.7238679518735981\n",
      "cost at iteration 2 = 0.6948694248669677\n",
      "cost at iteration 3 = 0.668881489663552\n",
      "cost at iteration 4 = 0.645025065297518\n",
      "cost at iteration 5 = 0.6227083300956007\n",
      "cost at iteration 6 = 0.601529884734578\n",
      "cost at iteration 7 = 0.5812182079842272\n",
      "cost at iteration 8 = 0.561592514216844\n",
      "cost at iteration 9 = 0.5425364305790784\n",
      "cost at iteration 10 = 0.5239795625880667\n",
      "cost at iteration 11 = 0.5058840988463813\n",
      "cost at iteration 12 = 0.4882347954692656\n",
      "cost at iteration 13 = 0.4710313622675012\n",
      "cost at iteration 14 = 0.4542826593739638\n",
      "cost at iteration 15 = 0.4380023264003614\n",
      "cost at iteration 16 = 0.42220557853099855\n",
      "cost at iteration 17 = 0.40690695995018467\n",
      "cost at iteration 18 = 0.3921188729157499\n",
      "cost at iteration 19 = 0.37785071790123553\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train,y_train,layers_dims,learning_rate=0.09, lambd=1.0,num_iterations = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = my_data_transformer.transform(X_test)\n",
    "\n",
    "y_test = np.array(y_test).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7752100840336135"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, _ = L_feed_forward(parameters,X_test)\n",
    "\n",
    "y_pred = np.where(y_pred>0.5,1,0)\n",
    "\n",
    "y_pred\n",
    "\n",
    "y_test\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test[0], y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9194254685584166"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, _ = L_feed_forward(parameters,X_train)\n",
    "\n",
    "y_pred = np.where(y_pred>0.5,1,0)\n",
    "\n",
    "y_pred\n",
    "\n",
    "y_train\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_train[0], y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
